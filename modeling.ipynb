{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to use Naive Bayes, Decision Tree, and K Nearest Neighbors classifiers to predict our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def nb_model(x_train, x_test, y_train, y_test, var_smoothing=1e-9):\n",
    "    model = GaussianNB(var_smoothing=var_smoothing)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    score = model.score(x_test, y_test)\n",
    "    return y_pred, score\n",
    "\n",
    "def tree_model(x_train, x_test, y_train, y_test, max_depth=None, criterion='gini'):\n",
    "    model = DecisionTreeClassifier(max_depth=max_depth, criterion=criterion)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    score = model.score(x_test, y_test)\n",
    "    return y_pred, score\n",
    "\n",
    "\n",
    "def knn_model(x_train, x_test, y_train, y_test, weights='uniform', leaf_size=30, p=2):\n",
    "    model = KNeighborsClassifier(weights=weights, leaf_size=leaf_size, p=p)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    score = model.score(x_test, y_test)\n",
    "    return y_pred, score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is split for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('data/speed_dating_select.csv')\n",
    "target = 'match'\n",
    "desc_features = list(data.columns)\n",
    "desc_features.remove(target)\n",
    "\n",
    "x = data[desc_features]\n",
    "y = data[target]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization (Grid Search with Accuracy Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log loss parameter was introdiced in scikit-learn version 1.1.2; please make sure your scikit-learn package is up to date before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum score 0.8509822712026833 at depth 13 using gini\n"
     ]
    }
   ],
   "source": [
    "def optimize_tree():\n",
    "    depths = []\n",
    "    crits = []\n",
    "    scores = []\n",
    "    for depth in [i for i in range(1,15)]:\n",
    "        for crit in ['gini', 'entropy', 'log_loss']:\n",
    "            _, score = tree_model(x_train, x_test, y_train, y_test, max_depth=depth, criterion=crit)\n",
    "            scores.append(score)\n",
    "            depths.append(depth)\n",
    "            crits.append(crit)\n",
    "    best = scores.index(max(scores))\n",
    "    print(f'Maximum score {scores[best]} at depth {depths[best]} using {crits[best]}')\n",
    "    return {\n",
    "        'max_depth': depths[best],\n",
    "        'criterion': crits[best]\n",
    "    }\n",
    "\n",
    "tree_params = optimize_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum score 0.8112122664111164 with weights uniform, leaf size 10 and p=1\n"
     ]
    }
   ],
   "source": [
    "def optimize_knn():\n",
    "    weights = []\n",
    "    leaf_sizes = []\n",
    "    ps = []\n",
    "    scores = []\n",
    "    for weight in ['uniform', 'distance']:\n",
    "        for p in [1,2,3,4]:\n",
    "            for leaf_size in [10, 30, 50, 100]:\n",
    "                _, score = knn_model(x_train, x_test, y_train, y_test, weights=weight, leaf_size=leaf_size, p=p)\n",
    "                scores.append(score)\n",
    "                weights.append(weight)\n",
    "                leaf_sizes.append(leaf_size)\n",
    "                ps.append(p)\n",
    "    best = scores.index(max(scores))\n",
    "    print(f'Maximum score {scores[best]} with weights {weights[best]}, leaf size {leaf_sizes[best]} and p={ps[best]}')\n",
    "    return {\n",
    "        'weights': weights[best],\n",
    "        'leaf_size': leaf_sizes[best],\n",
    "        'p': ps[best]\n",
    "    }\n",
    "\n",
    "knn_params = optimize_knn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum score 0.8279827503593675 with smoothing 0.1\n"
     ]
    }
   ],
   "source": [
    "def optimize_nb():\n",
    "    smooths = []\n",
    "    scores = []\n",
    "    for smoothing in [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10, 1e-11]:\n",
    "        _, score = nb_model(x_train, x_test, y_train, y_test, var_smoothing=smoothing)\n",
    "        smooths.append(smoothing)\n",
    "        scores.append(score)\n",
    "    best = scores.index(max(scores))\n",
    "    print(f'Maximum score {scores[best]} with smoothing {smooths[best]}')\n",
    "    return {\n",
    "        'var_smoothing': smooths[best]\n",
    "    }\n",
    "\n",
    "nb_params = optimize_nb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8279827503593675\n",
      "0.8457115476760901\n",
      "0.8112122664111164\n"
     ]
    }
   ],
   "source": [
    "y_pred, score = nb_model(x_train, x_test, y_train, y_test, **nb_params)\n",
    "\n",
    "print(score)\n",
    "\n",
    "y_pred, score = tree_model(x_train, x_test, y_train, y_test, **tree_params)\n",
    "\n",
    "print(score)\n",
    "\n",
    "y_pred, score = knn_model(x_train, x_test, y_train, y_test, **knn_params)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
