{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to use Naive Bayes, Decision Tree, and K Nearest Neighbors classifiers to predict our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def nb_model(x_train, x_test, y_train, y_test, var_smoothing=1e-9):\n",
    "    model = GaussianNB(var_smoothing=var_smoothing)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict_proba(x_test)[:,1]\n",
    "    score = model.score(x_test, y_test)\n",
    "    return y_pred, score\n",
    "\n",
    "def tree_model(x_train, x_test, y_train, y_test, max_depth=None, criterion='gini'):\n",
    "    model = DecisionTreeClassifier(max_depth=max_depth, criterion=criterion)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict_proba(x_test)[:,1]\n",
    "    score = model.score(x_test, y_test)\n",
    "    return y_pred, score\n",
    "\n",
    "\n",
    "def knn_model(x_train, x_test, y_train, y_test, weights='uniform', leaf_size=30, p=2):\n",
    "    model = KNeighborsClassifier(weights=weights, leaf_size=leaf_size, p=p)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict_proba(x_test)[:,1]\n",
    "    score = model.score(x_test, y_test)\n",
    "    return y_pred, score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is split for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('data/cupid_scientists_pp.csv')\n",
    "target = 'match'\n",
    "desc_features = list(data.columns)\n",
    "desc_features.remove(target)\n",
    "\n",
    "x = data[desc_features]\n",
    "y = data[target]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization (Grid Search with Accuracy Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log loss parameter was introdiced in scikit-learn version 1.1.2. If you encounter problems, please make sure your scikit-learn package is up to date before running this code. `pip install scikit-learn==1.1.3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum score 0.8466698610445615 at depth 14 using entropy\n"
     ]
    }
   ],
   "source": [
    "def optimize_tree():\n",
    "    depths = []\n",
    "    crits = []\n",
    "    scores = []\n",
    "    for depth in [i for i in range(1,15)]:\n",
    "        for crit in ['gini', 'entropy', 'log_loss']:\n",
    "            _, score = tree_model(x_train, x_test, y_train, y_test, max_depth=depth, criterion=crit)\n",
    "            scores.append(score)\n",
    "            depths.append(depth)\n",
    "            crits.append(crit)\n",
    "    best = scores.index(max(scores))\n",
    "    print(f'Maximum score {scores[best]} at depth {depths[best]} using {crits[best]}')\n",
    "    return {\n",
    "        'max_depth': depths[best],\n",
    "        'criterion': crits[best]\n",
    "    }\n",
    "\n",
    "tree_params = optimize_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum score 0.8667944417824629 with weights distance, leaf size 10 and p=1\n"
     ]
    }
   ],
   "source": [
    "def optimize_knn():\n",
    "    weights = []\n",
    "    leaf_sizes = []\n",
    "    ps = []\n",
    "    scores = []\n",
    "    for weight in ['uniform', 'distance']:\n",
    "        for p in [1,2,3,4]:\n",
    "            for leaf_size in [10, 30, 50, 100]:\n",
    "                _, score = knn_model(x_train, x_test, y_train, y_test, weights=weight, leaf_size=leaf_size, p=p)\n",
    "                scores.append(score)\n",
    "                weights.append(weight)\n",
    "                leaf_sizes.append(leaf_size)\n",
    "                ps.append(p)\n",
    "    best = scores.index(max(scores))\n",
    "    print(f'Maximum score {scores[best]} with weights {weights[best]}, leaf size {leaf_sizes[best]} and p={ps[best]}')\n",
    "    return {\n",
    "        'weights': weights[best],\n",
    "        'leaf_size': leaf_sizes[best],\n",
    "        'p': ps[best]\n",
    "    }\n",
    "\n",
    "knn_params = optimize_knn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum score 0.8279827503593675 with smoothing 0.1\n"
     ]
    }
   ],
   "source": [
    "def optimize_nb():\n",
    "    smooths = []\n",
    "    scores = []\n",
    "    for smoothing in [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10, 1e-11]:\n",
    "        _, score = nb_model(x_train, x_test, y_train, y_test, var_smoothing=smoothing)\n",
    "        smooths.append(smoothing)\n",
    "        scores.append(score)\n",
    "    best = scores.index(max(scores))\n",
    "    print(f'Maximum score {scores[best]} with smoothing {smooths[best]}')\n",
    "    return {\n",
    "        'var_smoothing': smooths[best]\n",
    "    }\n",
    "\n",
    "nb_params = optimize_nb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on accuracy alone, the decision tree model seems capable of the best results. However, we wanted to dive deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_conf_matrix(observation, p_scores, threshold):\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for i in range(len(observation)):\n",
    "        prediction = (1 if p_scores[i] >= threshold else 0)\n",
    "        tf = (True if prediction == observation[i] else False)\n",
    "        pn = (True if prediction == 1 else False)\n",
    "        if tf and pn:\n",
    "            TP += 1\n",
    "        if tf and not pn:\n",
    "            TN += 1\n",
    "        if not tf and pn:\n",
    "            FP += 1\n",
    "        if not tf and not pn:\n",
    "            FN += 1\n",
    "\n",
    "    out = {\n",
    "        'TP': TP,\n",
    "        'TN': TN,\n",
    "        'FP': FP,\n",
    "        'FN': FN\n",
    "    }\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent of correct predictions\n",
    "def accuracy(observation, p_scores, threshold):\n",
    "    # (TP + TN) / (TP + TN + FP + FN)\n",
    "    scores = binary_conf_matrix(observation, p_scores, threshold)\n",
    "    out = scores['TP'] + scores['TN']\n",
    "    out /= (scores['TP'] + scores['TN'] + scores['FP'] + scores['FN'])\n",
    "    return out\n",
    "\n",
    "# percent of predicted positives that were correct\n",
    "def precision(observation, p_scores, threshold):\n",
    "    # TP / (TP + FP)\n",
    "    scores = binary_conf_matrix(observation, p_scores, threshold)\n",
    "    out = scores['TP']\n",
    "    out /= (scores['TP'] + scores['FP'])\n",
    "    return out\n",
    "\n",
    "# percent of real positives correctly predicted\n",
    "def recall(observation, p_scores, threshold):\n",
    "    # TP / (TP + FN)\n",
    "    scores = binary_conf_matrix(observation, p_scores, threshold)\n",
    "    out = scores['TP']\n",
    "    out /= (scores['TP'] + scores['FN'])\n",
    "    return out\n",
    "\n",
    "# f1 score\n",
    "def f1score(observation, p_scores, threshold):\n",
    "    # 2(precision * recall) / (precision + recall)\n",
    "    prec = precision(observation, p_scores, threshold)\n",
    "    rec = recall(observation, p_scores, threshold)\n",
    "    out = 2 * (prec * rec)\n",
    "    out /= (prec + rec)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hanssen-Kuipers Skill Score: How well the model separates matches from non-matches\n",
    "def tss(observation, p_scores, threshold):\n",
    "    # (TP / (TP + FN)) - (FP / (FP + TN))\n",
    "    scores = binary_conf_matrix(observation, p_scores, threshold)\n",
    "    first = scores['TP']\n",
    "    first /= (scores['TP'] + scores['FN'])\n",
    "    second = scores['FP']\n",
    "    second /= (scores['FP'] + scores['TN'])\n",
    "    out = first - second\n",
    "    return out\n",
    "\n",
    "# Gilbert Skill Score: How well predicted matches correspond to true matches, accounting for lucky random predictions\n",
    "def gss(observation, p_scores, threshold):\n",
    "    # random hits = ((TP + FN) * (TP + FP)) / (TP + FP + TN + FN)\n",
    "    # gss = (TP - random) / (TP + FN + FP - random)\n",
    "    scores = binary_conf_matrix(observation, p_scores, threshold)\n",
    "    hits_random = scores['TP'] + scores['FN']\n",
    "    hits_random *= (scores['TP'] + scores['FP'])\n",
    "    hits_random /= (scores['TP'] + scores['FP'] + scores['TN'] + scores['FN'])\n",
    "    out = scores['TP'] - hits_random\n",
    "    out /= (scores['TP'] + scores['FN'] + scores['FP'] - hits_random)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another element of hyperparameter optimization, we can pick the best threshold for prediction to apply with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick best threshold based on a specific metric\n",
    "def pick_threshold(observation, p_scores, mfunc):\n",
    "    performances = []\n",
    "    for threshold in [i/100 for i in range(0,101)]:\n",
    "        try:\n",
    "            perf = mfunc(observation, p_scores, threshold)\n",
    "        except ZeroDivisionError: # encountered with FAR score\n",
    "            perf = -1\n",
    "        performances.append(perf)\n",
    "    best_index = performances.index(max(performances))\n",
    "    best_threshold = best_index / 100\n",
    "    return best_threshold\n",
    "\n",
    "def pick_threshold_all_metrics(observation, p_scores, mfuncs=[accuracy, f1score, tss, gss]):\n",
    "    # precision and recall not included because they will automatically bias towards thresholds of 0 and 1\n",
    "    thresholds = []\n",
    "    for mfunc in mfuncs:\n",
    "        threshold = pick_threshold(observation, p_scores, mfunc)\n",
    "        thresholds.append(threshold)\n",
    "    return thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(observation, p_scores, threshold):\n",
    "    metrics = {\n",
    "        'accuracy': accuracy(observation, p_scores, threshold),\n",
    "        'precision': precision(observation, p_scores, threshold),\n",
    "        'recall': recall(observation, p_scores, threshold),\n",
    "        'f1score': f1score(observation, p_scores, threshold),\n",
    "        'tss': tss(observation, p_scores, threshold),\n",
    "        'gss': gss(observation, p_scores, threshold),\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without even looking at the further metrics, we see that the Naive Bayes classifier is actually just getting its decent accuracy score by picking a negative match 100% of the time. This is definitely not what we want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TP': 0, 'TN': 1728, 'FP': 0, 'FN': 359}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_scores, score = nb_model(x_train, x_test, y_train, y_test, **nb_params)\n",
    "# print(pick_threshold_all_metrics(y_test, p_scores))\n",
    "\n",
    "binary_conf_matrix(list(y_test), p_scores, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the K Neighbors model produces a good accuracy, it scores much lower on all other metrics than the decision tree does. It also claims to do best with thresholds of of 0.01, meaning simply picking almost every match as successful with no refusals, which is not what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.42, 0.42, 0.21, 0.42]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8667944417824629,\n",
       " 'precision': 0.7165775401069518,\n",
       " 'recall': 0.3732590529247911,\n",
       " 'f1score': 0.49084249084249093,\n",
       " 'tss': 0.34258775662849483,\n",
       " 'gss': 0.26809896671044053}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_scores, score = knn_model(x_train, x_test, y_train, y_test, **knn_params)\n",
    "\n",
    "print(pick_threshold_all_metrics(list(y_test), p_scores))\n",
    "get_metrics(list(y_test), p_scores, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree's scores look much better and much more robust.\n",
    "\n",
    "Asking for the best threshold per metric returns `[0.95, 0.28, 0.28, 0.29]` (sometimes these numbers are slightly different, depending on the parameters selected above, but the last 3 generally average around 0.28 altogether). The high first number is when tested with accuracy; because this dataset has far more failed matches than successful ones, so pure accuracy would be biased towards the negative result (high threshold). However, the other three favor a lower threshold, so that's what we consider for the threshold for the following metrics.\n",
    "\n",
    "With this, we retain good accuracy (around 0.80) while having high other metrics. Additionally, we are more biased towards positive matches, which is a good thing -- we would prefer to have a possible match to show a user than not have anything to show at all when someone uses this app.\n",
    "\n",
    "Thus, we will recommend the Decision Tree model, with the hyperparameters selected above and a decision threshold being an average between the best for F1 Score, TSS, and GSS, (so usually around 0.28) for the client's app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.67, 0.29, 0.12, 0.41]\n",
      "Selected best threshold based on F1 Score, TSS, and GSS: 0.2733333333333333\n",
      "{'TP': 154, 'TN': 1511, 'FP': 217, 'FN': 205}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7977958792525156,\n",
       " 'precision': 0.41509433962264153,\n",
       " 'recall': 0.42896935933147634,\n",
       " 'f1score': 0.42191780821917807,\n",
       " 'tss': 0.3033906556277727,\n",
       " 'gss': 0.1760734870519205}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_scores, score = tree_model(x_train, x_test, y_train, y_test, **tree_params)\n",
    "\n",
    "thresholds = pick_threshold_all_metrics(list(y_test), p_scores)\n",
    "print(thresholds)\n",
    "\n",
    "avg_best_thresh = (thresholds[1] + thresholds[2] + thresholds[3]) / 3\n",
    "print(f'Selected best threshold based on F1 Score, TSS, and GSS: {avg_best_thresh}')\n",
    "\n",
    "print(binary_conf_matrix(list(y_test), p_scores, avg_best_thresh))\n",
    "\n",
    "get_metrics(list(y_test), p_scores, avg_best_thresh)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
